{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7499077,"sourceType":"datasetVersion","datasetId":4366744},{"sourceId":172586063,"sourceType":"kernelVersion"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-19T14:45:26.952291Z","iopub.execute_input":"2024-04-19T14:45:26.952649Z","iopub.status.idle":"2024-04-19T14:45:26.957848Z","shell.execute_reply.started":"2024-04-19T14:45:26.952621Z","shell.execute_reply":"2024-04-19T14:45:26.956898Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"subjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# First, let's identify the unique groups in the DataFrame\ngroups = subjects_info['Group'].unique()\ngroups=[\"A\",\"C\"]\n# Now, let's split each group individually\ntrain_dfs = []\ntest_dfs = []\ntotal=[]\nfor group in groups:\n    # Filter the DataFrame for the current group\n    group_df = subjects_info[subjects_info['Group'] == group]\n    \n    # Split the group data into training and testing sets while maintaining balance in gender\n    train_group, test_group = train_test_split(group_df, test_size=0.3, stratify=group_df['Gender'], random_state=42)\n    total.append(group_df)\n    # Append the split data to the lists\n    train_dfs.append(train_group)\n    test_dfs.append(test_group)\n\n# Concatenate the training and testing DataFrames for all groups\ntrain_df = pd.concat(train_dfs)\ntest_df = pd.concat(test_dfs)\ntotal_df=pd.concat(total)\n# Now, train_df and test_df contain the split data with balanced groups and secondary balance in gender\n# Extracting subject IDs from the training set\ntraining_subjects = train_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n\n# Extracting subject IDs from the testing set\ntesting_subjects = test_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n\ntotal_subjects = total_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n\n# Displaying the lists of subjects\nprint(\"Training Subjects:\")\nprint(training_subjects)\n\nprint(\"\\nTesting Subjects:\")\nprint(testing_subjects)\n\nprint(\"\\nTotal Subjects:\")\nprint(total_subjects)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:45:27.135718Z","iopub.execute_input":"2024-04-19T14:45:27.136220Z","iopub.status.idle":"2024-04-19T14:45:27.168191Z","shell.execute_reply.started":"2024-04-19T14:45:27.136196Z","shell.execute_reply":"2024-04-19T14:45:27.167402Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training Subjects:\n[17, 18, 10, 35, 6, 2, 5, 4, 27, 9, 3, 14, 30, 31, 29, 15, 34, 25, 8, 22, 1, 23, 24, 11, 7, 58, 41, 44, 51, 50, 49, 45, 37, 53, 65, 60, 38, 61, 62, 57, 64, 54, 59, 55, 46]\n\nTesting Subjects:\n[28, 12, 26, 21, 36, 19, 33, 32, 16, 20, 13, 52, 63, 48, 43, 47, 39, 40, 56, 42]\n\nTotal Subjects:\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"##laods the data\n\nimport numpy as np\nfrom scipy.io import loadmat\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef load_data(subjects_info,subjects, data_type='training'):\n    groups=[]\n    morlet = []\n    rbps = []\n    scc=[]\n    labels = []\n    for idx in subjects:\n        file_path_morlet = f\"/kaggle/input/time-freq-transforms/cleaned_data_og/morlet_transform/subject_{idx}.npy\"\n        subject_data_morlet = np.load(file_path_morlet)\n        \n        morlet.append(subject_data_morlet)\n        \n#         file_path_rbp = f\"/kaggle/input/extraction-cleaned-custom-cleaned/rbp/rbp_{idx}.npy\"\n#         subject_rbp = np.load(file_path_rbp)\n        \n#         rbps.append(subject_rbp)\n        \n#         file_path_scc = f\"/kaggle/input/extraction-cleaned-custom-cleaned/scc_cleaned_base/subject_{idx}_scc.npy\"\n#         subject_scc = np.load(file_path_scc)\n        \n#         scc.append(subject_scc)\n        \n        # Duplicate entries in the 'Group' column\n        num_epochs = subject_data_morlet.shape[0]  # Number of epochs, replace 'your_data_key' as before\n        subject_id = f\"sub-{str(idx).zfill(3)}\"  # Format subject ID\n        group_info = subjects_info[subjects_info['participant_id'] == subject_id]['Group'].values[0]\n        duplicated_groups = [group_info] * num_epochs\n        labels.extend(duplicated_groups)\n        \n        groups.extend([idx] * num_epochs)\n        \n    return np.concatenate(morlet, axis=0),labels,groups","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:45:27.311536Z","iopub.execute_input":"2024-04-19T14:45:27.311826Z","iopub.status.idle":"2024-04-19T14:45:27.319637Z","shell.execute_reply.started":"2024-04-19T14:45:27.311802Z","shell.execute_reply":"2024-04-19T14:45:27.318804Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"###source file\nsubjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')\n##########","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:45:28.617760Z","iopub.execute_input":"2024-04-19T14:45:28.618679Z","iopub.status.idle":"2024-04-19T14:45:28.625100Z","shell.execute_reply.started":"2024-04-19T14:45:28.618646Z","shell.execute_reply":"2024-04-19T14:45:28.624318Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"morlet_total, total_labels, groups_total = load_data(subjects_info, total_subjects, data_type='total')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:45:29.485699Z","iopub.execute_input":"2024-04-19T14:45:29.486561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you define label_mapping, encoder, and encode labels as in your code\n#####################################################\nlabel_mapping = {'A': 0, 'C': 1, 'F': 2}  # You can adjust this mapping as needed\n#####################################################\ndf=pd.Series(total_labels).map(label_mapping)\n\nencoder = OneHotEncoder(categories='auto', sparse=False)\nencoder.fit(np.array(pd.Series(total_labels).map(label_mapping)).reshape(-1, 1))\n# Encode labels\n################################################\n\ntotal_labels_encoded = encoder.transform(np.array(pd.Series(total_labels).map(label_mapping).values).reshape(-1, 1))\n################################################\n\n\nprint(\"Total labels shape:\", total_labels_encoded.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n#####total dataset \nmorlet_tensor_total = torch.tensor(morlet_total, dtype=torch.float32)\nlabels_tensor_total = torch.tensor(total_labels_encoded, dtype=torch.float32)\n\ndataset_total = TensorDataset(morlet_tensor_total, labels_tensor_total)\n\nbatch_size = 64\n\ntrain_loader = DataLoader(dataset_total, batch_size=batch_size, shuffle=True,drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:37:50.836205Z","iopub.execute_input":"2024-04-19T14:37:50.836554Z","iopub.status.idle":"2024-04-19T14:37:52.247128Z","shell.execute_reply.started":"2024-04-19T14:37:50.836526Z","shell.execute_reply":"2024-04-19T14:37:52.246071Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\n# Step 1: Define the modified ResNet model\nclass ModifiedResNet(nn.Module):\n    def __init__(self, num_classes):\n        super(ModifiedResNet, self).__init__()\n        # Load the pre-trained ResNet model\n        resnet = models.resnet50(pretrained=True)\n        \n        # Modify the input layer\n        resnet.conv1 = nn.Conv2d(19, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Modify the output layer\n        resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n        \n        # Freeze the parameters of the pre-trained layers\n        for param in resnet.parameters():\n            param.requires_grad = True\n        \n        # Make the parameters of the modified layers trainable\n        for param in resnet.fc.parameters():\n            param.requires_grad = True\n        \n        self.resnet = resnet\n\n    def forward(self, x):\n        return self.resnet(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:28:26.044419Z","iopub.status.idle":"2024-04-19T14:28:26.044737Z","shell.execute_reply.started":"2024-04-19T14:28:26.044581Z","shell.execute_reply":"2024-04-19T14:28:26.044594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.metrics import accuracy_score\n\n# Load your dataset\n\n# Assuming your dataset is in the form of a list of dictionaries\n# where each dictionary represents a subject with data, labels, and a group identifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 256\nlogo = LeaveOneGroupOut()\ngroup_indices = logo.split(np.arange(len(dataset_total)), groups=groups_total)\ntotal_y_true = []\ntotal_y_pred = []\n\nfor train_indices, test_indices in group_indices:\n    # Split the dataset into train and test sets based on group indices\n    train_data = [dataset_total[i] for i in train_indices]\n    test_data = [dataset_total[i] for i in test_indices]\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n    # Initialize the model and optimizer\n    model = ModifiedResNet(2).to(device)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Early stopping parameters\n    patience = 2# Number of epochs to wait before early stopping\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    best_model_state = None  # Variable to store the best model's state\n\n    # Training loop\n    num_epochs = 150\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs1, labels in train_loader:\n            optimizer.zero_grad()\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Calculate validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs1, labels in test_loader:\n                inputs1 = inputs1.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs1)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        val_loss /= len(test_loader)\n        print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}\")\n\n        # Early stopping and save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()  # Save the best model's state\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n\n        if early_stop_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    # Load the best model's state\n    model.load_state_dict(best_model_state)\n\n    # Evaluate on the test set\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs1, labels in test_loader:\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(predicted.cpu().tolist())\n\n    total_y_true.extend(labels.cpu().tolist())\n    total_y_pred.extend(predicted.cpu().tolist())\n\n# Calculate classification report\nreport = classification_report(np.argmax(y_true, axis=1), y_pred)\nprint(\"Classification Report:\")\nprint(report)\nprint(\"\\n\")\n\nreport = classification_report(np.argmax(total_y_true, axis=1), total_y_pred)\nprint(\"total_Classification Report:\")\nprint(report)\nprint(\"\\n\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-19T13:20:06.406949Z","iopub.execute_input":"2024-04-19T13:20:06.407689Z","iopub.status.idle":"2024-04-19T13:55:26.367515Z","shell.execute_reply.started":"2024-04-19T13:20:06.407653Z","shell.execute_reply":"2024-04-19T13:55:26.366413Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 0.7315985219819205, Val Loss: 6.187893867492676\nEpoch 2, Train Loss: 3.4689119287899564, Val Loss: 0.2843853235244751\nEpoch 3, Train Loss: 0.7141327857971191, Val Loss: 1.0053882598876953\nEpoch 4, Train Loss: 0.7050550920622689, Val Loss: 0.6610081791877747\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.7091161438396999, Val Loss: 0.09184339642524719\nEpoch 2, Train Loss: 4.278968410832541, Val Loss: 1.3558499813079834\nEpoch 3, Train Loss: 0.98891282081604, Val Loss: 0.44086745381355286\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.723374034677233, Val Loss: 15.476301193237305\nEpoch 2, Train Loss: 20.18958272252764, Val Loss: 0.7087131142616272\nEpoch 3, Train Loss: 0.9738549760409764, Val Loss: 0.731797993183136\nEpoch 4, Train Loss: 0.7014114090374538, Val Loss: 0.6141326427459717\nEpoch 5, Train Loss: 0.7049615383148193, Val Loss: 0.6544997096061707\nEpoch 6, Train Loss: 0.6917514375277928, Val Loss: 0.5927849411964417\nEpoch 7, Train Loss: 0.6910059026309422, Val Loss: 0.5945004224777222\nEpoch 8, Train Loss: 0.689499488898686, Val Loss: 0.6191688179969788\nEarly stopping at epoch 8\nEpoch 1, Train Loss: 0.7293246729033334, Val Loss: 0.011889427900314331\nEpoch 2, Train Loss: 2.6618348445211137, Val Loss: 0.48611608147621155\nEpoch 3, Train Loss: 0.7399461184229169, Val Loss: 0.7628211975097656\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7284724201474871, Val Loss: 8.662172317504883\nEpoch 2, Train Loss: 3.3987805247306824, Val Loss: 0.254852294921875\nEpoch 3, Train Loss: 0.8112857597214835, Val Loss: 0.5914589166641235\nEpoch 4, Train Loss: 0.712583235331944, Val Loss: 0.8535404801368713\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6687994258744376, Val Loss: 0.0\nEpoch 2, Train Loss: 4.6676174231937955, Val Loss: 1.8536655902862549\nEpoch 3, Train Loss: 0.8045806799616132, Val Loss: 0.6662812829017639\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7257397430283683, Val Loss: 0.2893930673599243\nEpoch 2, Train Loss: 4.392082742282322, Val Loss: 1.1105796098709106\nEpoch 3, Train Loss: 1.0934356706483024, Val Loss: 0.2936977446079254\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.739084712096623, Val Loss: 2.145764938177308e-06\nEpoch 2, Train Loss: 5.163100481033325, Val Loss: 6.330679893493652\nEpoch 3, Train Loss: 1.0866278750555856, Val Loss: 0.45597997307777405\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7933255348886762, Val Loss: 0.0\nEpoch 2, Train Loss: 479.7666166509901, Val Loss: 1.1743777990341187\nEpoch 3, Train Loss: 0.9515908701079232, Val Loss: 0.42230597138404846\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.771487431866782, Val Loss: 26.030424118041992\nEpoch 2, Train Loss: 3.9901358910969327, Val Loss: 0.19729401171207428\nEpoch 3, Train Loss: 0.8033958928925651, Val Loss: 0.6873507499694824\nEpoch 4, Train Loss: 0.7104081170899528, Val Loss: 0.7672498226165771\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6711744240352085, Val Loss: 1.189701561088441e-05\nEpoch 2, Train Loss: 22.28425317151206, Val Loss: 0.2757126986980438\nEpoch 3, Train Loss: 0.7824344890458244, Val Loss: 0.7091244459152222\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.8359836850847516, Val Loss: 457.3512268066406\nEpoch 2, Train Loss: 44.73233573777335, Val Loss: 3.490874767303467\nEpoch 3, Train Loss: 0.9382995026452201, Val Loss: 0.1817285418510437\nEpoch 4, Train Loss: 0.7615852185658046, Val Loss: 0.4773971438407898\nEpoch 5, Train Loss: 0.6928016798836845, Val Loss: 1.3863152265548706\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.6532805647168841, Val Loss: 0.0\nEpoch 2, Train Loss: 32.570243656635284, Val Loss: 0.06725840270519257\nEpoch 3, Train Loss: 0.8211109382765633, Val Loss: 0.8164122700691223\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7308824019772666, Val Loss: 3.277008533477783\nEpoch 2, Train Loss: 1.95152975831713, Val Loss: 0.7013297080993652\nEpoch 3, Train Loss: 0.7381658128329686, Val Loss: 0.3512727618217468\nEpoch 4, Train Loss: 0.6997153844152179, Val Loss: 0.7192707657814026\nEpoch 5, Train Loss: 0.6925939662115914, Val Loss: 0.5235210061073303\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.8002281955310276, Val Loss: 1294.31005859375\nEpoch 2, Train Loss: 90.17682532753263, Val Loss: 1.0901471376419067\nEpoch 3, Train Loss: 0.748238205909729, Val Loss: 0.7979403734207153\nEpoch 4, Train Loss: 0.7019415327480861, Val Loss: 0.7573265433311462\nEpoch 5, Train Loss: 0.6997198803084237, Val Loss: 0.7906617522239685\nEpoch 6, Train Loss: 0.9952683789389474, Val Loss: 0.1436927765607834\nEpoch 7, Train Loss: 0.7940474918910435, Val Loss: 0.6144611835479736\nEpoch 8, Train Loss: 0.69435567515237, Val Loss: 0.6511976718902588\nEarly stopping at epoch 8\nEpoch 1, Train Loss: 0.7383539080619812, Val Loss: 0.10735931247472763\nEpoch 2, Train Loss: 1116.0284768002373, Val Loss: 0.6430826783180237\nEpoch 3, Train Loss: 1.297705956867763, Val Loss: 0.4469611644744873\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6835307053157261, Val Loss: 0.011241131462156773\nEpoch 2, Train Loss: 81.9841131738254, Val Loss: 3.3684329986572266\nEpoch 3, Train Loss: 0.954449611050742, Val Loss: 0.25628653168678284\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7432836805071149, Val Loss: 0.0\nEpoch 2, Train Loss: 83.93956360646656, Val Loss: 1.3193459510803223\nEpoch 3, Train Loss: 0.7779333336012704, Val Loss: 0.3382522463798523\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7717148065567017, Val Loss: 1.4843255281448364\nEpoch 2, Train Loss: 12.67683915581022, Val Loss: 0.3851337134838104\nEpoch 3, Train Loss: 0.8105524012020656, Val Loss: 1.1244405508041382\nEpoch 4, Train Loss: 0.7312284793172564, Val Loss: 0.5823597311973572\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.764723641531808, Val Loss: 546.5700073242188\nEpoch 2, Train Loss: 48.0232743195125, Val Loss: 0.7073191404342651\nEpoch 3, Train Loss: 0.8573015757969448, Val Loss: 0.2582647204399109\nEpoch 4, Train Loss: 0.7173811708177839, Val Loss: 0.5506810545921326\nEpoch 5, Train Loss: 0.6938238058771405, Val Loss: 0.5754274725914001\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.6923860056059701, Val Loss: 11.88101863861084\nEpoch 2, Train Loss: 3.041214976991926, Val Loss: 2.0606894493103027\nEpoch 3, Train Loss: 0.8022652609007699, Val Loss: 0.37342050671577454\nEpoch 4, Train Loss: 0.7014874219894409, Val Loss: 1.0764555931091309\nEpoch 5, Train Loss: 0.7091637934957232, Val Loss: 0.6984385251998901\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.7588510598455157, Val Loss: 0.0\nEpoch 2, Train Loss: 29.994315726416453, Val Loss: 0.07889485359191895\nEpoch 3, Train Loss: 0.8581067408834185, Val Loss: 0.6395148038864136\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7796974607876369, Val Loss: 0.0\nEpoch 2, Train Loss: 22.332653794969833, Val Loss: 0.2758519649505615\nEpoch 3, Train Loss: 1.6042821833065577, Val Loss: 0.5143573880195618\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6877298269953046, Val Loss: 19.82349967956543\nEpoch 2, Train Loss: 4.715290316513607, Val Loss: 1.1658744812011719\nEpoch 3, Train Loss: 0.7842898028237479, Val Loss: 0.43288952112197876\nEpoch 4, Train Loss: 0.7025251473699298, Val Loss: 0.5868479609489441\nEpoch 5, Train Loss: 0.6945067303521293, Val Loss: 0.4660693407058716\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.7468828133174351, Val Loss: 0.00020339370530564338\nEpoch 2, Train Loss: 2.390952638217381, Val Loss: 1.0940665006637573\nEpoch 3, Train Loss: 0.7632861903735569, Val Loss: 0.22643055021762848\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6780578153473991, Val Loss: 1.6509160995483398\nEpoch 2, Train Loss: 572.2272610749517, Val Loss: 0.16157075762748718\nEpoch 3, Train Loss: 1.0654680984360831, Val Loss: 0.7942698001861572\nEpoch 4, Train Loss: 0.7659572618348258, Val Loss: 0.9155311584472656\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6620191931724548, Val Loss: 8.714070320129395\nEpoch 2, Train Loss: 2.2899446061679294, Val Loss: 0.3663099706172943\nEpoch 3, Train Loss: 0.7090713552066258, Val Loss: 0.7273939251899719\nEpoch 4, Train Loss: 0.7022139770644051, Val Loss: 0.7126181125640869\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.7637401052883693, Val Loss: 26.475309371948242\nEpoch 2, Train Loss: 6.213429382869175, Val Loss: 0.011494744569063187\nEpoch 3, Train Loss: 0.9574337346213204, Val Loss: 0.3700776696205139\nEpoch 4, Train Loss: 0.7028765337807792, Val Loss: 0.7259050607681274\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.7674299819128854, Val Loss: 0.0\nEpoch 2, Train Loss: 16.553679977144515, Val Loss: 1.752630591392517\nEpoch 3, Train Loss: 0.8913426739828927, Val Loss: 0.33724045753479004\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6144302231924874, Val Loss: 8.287443161010742\nEpoch 2, Train Loss: 3.548400955540793, Val Loss: 0.3718607425689697\nEpoch 3, Train Loss: 0.7459728036608014, Val Loss: 0.3778917193412781\nEpoch 4, Train Loss: 0.7062799334526062, Val Loss: 0.6134620308876038\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.678032751594271, Val Loss: 0.0\nEpoch 2, Train Loss: 9.7512794477599, Val Loss: 1.2698951959609985\nEpoch 3, Train Loss: 0.7865028721945626, Val Loss: 0.815831184387207\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6774594868932452, Val Loss: 0.0\nEpoch 2, Train Loss: 32.62925783225468, Val Loss: 0.5369369983673096\nEpoch 3, Train Loss: 0.7531525833266122, Val Loss: 1.626664638519287\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7191334877695356, Val Loss: 0.08043363690376282\nEpoch 2, Train Loss: 43.236075929233, Val Loss: 5.83107328414917\nEpoch 3, Train Loss: 1.2177482332502092, Val Loss: 0.851517379283905\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6963272775922503, Val Loss: 3.576278118089249e-07\nEpoch 2, Train Loss: 4.4668998803411215, Val Loss: 0.31568533182144165\nEpoch 3, Train Loss: 0.7556889823504856, Val Loss: 0.6446802020072937\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6595878430775234, Val Loss: 0.0\nEpoch 2, Train Loss: 97.94105069977897, Val Loss: 0.1585502177476883\nEpoch 3, Train Loss: 0.8263955116271973, Val Loss: 0.16044016182422638\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6984478916440692, Val Loss: 0.00023372443683911115\nEpoch 2, Train Loss: 5.750073271138327, Val Loss: 0.22607527673244476\nEpoch 3, Train Loss: 0.7612154654094151, Val Loss: 0.5030992031097412\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6894696056842804, Val Loss: 15.570387840270996\nEpoch 2, Train Loss: 3.300192551953452, Val Loss: 0.6960907578468323\nEpoch 3, Train Loss: 0.6955080628395081, Val Loss: 0.878406822681427\nEpoch 4, Train Loss: 0.7026783142771039, Val Loss: 0.8491328358650208\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.64366072842053, Val Loss: 0.0\nEpoch 2, Train Loss: 37.791862147195, Val Loss: 0.47174233198165894\nEpoch 3, Train Loss: 0.7492466143199376, Val Loss: 0.9627335071563721\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7073248965399606, Val Loss: 2.999613934662193e-05\nEpoch 2, Train Loss: 3.986994879586356, Val Loss: 0.8595209121704102\nEpoch 3, Train Loss: 0.9189543894359044, Val Loss: 0.4661923944950104\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.67597006048475, Val Loss: 122.72008514404297\nEpoch 2, Train Loss: 12.563042078699384, Val Loss: 0.7881746888160706\nEpoch 3, Train Loss: 1.8108165519578117, Val Loss: 0.968205451965332\nEpoch 4, Train Loss: 0.7236051389149257, Val Loss: 0.826410710811615\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6529711144311088, Val Loss: 14.112568855285645\nEpoch 2, Train Loss: 94.41723093816212, Val Loss: 1.1314082145690918\nEpoch 3, Train Loss: 0.7192860586302621, Val Loss: 0.29316094517707825\nEpoch 4, Train Loss: 0.7382906419890267, Val Loss: 1.029557704925537\nEpoch 5, Train Loss: 0.6912961517061506, Val Loss: 0.7426342368125916\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.6517835429736546, Val Loss: 2.0737063884735107\nEpoch 2, Train Loss: 51.82770834650312, Val Loss: 0.5534706711769104\nEpoch 3, Train Loss: 0.8603410295077732, Val Loss: 0.6365821361541748\nEpoch 4, Train Loss: 0.7748139585767474, Val Loss: 1.237976312637329\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6946416667529515, Val Loss: 6.695491313934326\nEpoch 2, Train Loss: 26.38181229148592, Val Loss: 9.502250671386719\nEpoch 3, Train Loss: 1.58875949042184, Val Loss: 0.7424129843711853\nEpoch 4, Train Loss: 0.6962251918656486, Val Loss: 0.7720414400100708\nEpoch 5, Train Loss: 0.6899516156741551, Val Loss: 0.6894977688789368\nEpoch 6, Train Loss: 0.6892523850713458, Val Loss: 0.6332724690437317\nEpoch 7, Train Loss: 0.6923564672470093, Val Loss: 0.6836537718772888\nEpoch 8, Train Loss: 0.6935778175081525, Val Loss: 0.7860736846923828\nEarly stopping at epoch 8\nEpoch 1, Train Loss: 0.657169657094138, Val Loss: 2.5223987102508545\nEpoch 2, Train Loss: 6.4790018286023825, Val Loss: 2.133512020111084\nEpoch 3, Train Loss: 0.7644975015095302, Val Loss: 0.6882741451263428\nEpoch 4, Train Loss: 0.7136066641126361, Val Loss: 0.637719988822937\nEpoch 5, Train Loss: 0.6905061517442975, Val Loss: 0.8108367323875427\nEpoch 6, Train Loss: 0.6901303785187858, Val Loss: 0.7840697765350342\nEarly stopping at epoch 6\nEpoch 1, Train Loss: 0.6362457914011819, Val Loss: 2.6909615993499756\nEpoch 2, Train Loss: 46.83960589340755, Val Loss: 0.8881635665893555\nEpoch 3, Train Loss: 1.6771526677267892, Val Loss: 0.736957848072052\nEpoch 4, Train Loss: 0.8214985983712333, Val Loss: 1.562678337097168\nEpoch 5, Train Loss: 0.7137238212994167, Val Loss: 0.7727766036987305\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.7928110020501273, Val Loss: 0.051453933119773865\nEpoch 2, Train Loss: 70.39072339875358, Val Loss: 1.7021993398666382\nEpoch 3, Train Loss: 0.8179922274180821, Val Loss: 0.5971734523773193\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7131837606430054, Val Loss: 32.97077560424805\nEpoch 2, Train Loss: 7.966406958443778, Val Loss: 0.9985734820365906\nEpoch 3, Train Loss: 0.9502865161214556, Val Loss: 1.174594759941101\nEpoch 4, Train Loss: 0.70414457150868, Val Loss: 0.9243993163108826\nEpoch 5, Train Loss: 0.691898626940591, Val Loss: 0.914669930934906\nEpoch 6, Train Loss: 0.6877561552183968, Val Loss: 0.7662800550460815\nEpoch 7, Train Loss: 0.6877510121890477, Val Loss: 0.7401033639907837\nEpoch 8, Train Loss: 0.6908211708068848, Val Loss: 0.8387764692306519\nEpoch 9, Train Loss: 0.686890721321106, Val Loss: 0.7656345367431641\nEarly stopping at epoch 9\nEpoch 1, Train Loss: 0.7128476074763707, Val Loss: 226.56707763671875\nEpoch 2, Train Loss: 24.816423552376882, Val Loss: 1.5322105884552002\nEpoch 3, Train Loss: 1.0864306773458208, Val Loss: 1.273624300956726\nEpoch 4, Train Loss: 0.6973719937460763, Val Loss: 0.7977645397186279\nEpoch 5, Train Loss: 0.690737077168056, Val Loss: 0.6100738048553467\nEpoch 6, Train Loss: 0.6923115168298993, Val Loss: 0.9041347503662109\nEpoch 7, Train Loss: 0.688603333064488, Val Loss: 0.8901326656341553\nEarly stopping at epoch 7\nEpoch 1, Train Loss: 0.683774756533759, Val Loss: 1.61972975730896\nEpoch 2, Train Loss: 4.008483290672302, Val Loss: 0.3701173663139343\nEpoch 3, Train Loss: 0.7188652328082493, Val Loss: 0.8131145238876343\nEpoch 4, Train Loss: 0.7179659179278782, Val Loss: 0.9538648724555969\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6498685479164124, Val Loss: 2.3841855067985307e-07\nEpoch 2, Train Loss: 3.9687937753541127, Val Loss: 0.5150138735771179\nEpoch 3, Train Loss: 0.8090398141316005, Val Loss: 0.9329690337181091\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.7313705257007054, Val Loss: 1145.749755859375\nEpoch 2, Train Loss: 73.23733144147056, Val Loss: 0.36607497930526733\nEpoch 3, Train Loss: 0.8265447786876133, Val Loss: 0.8037371635437012\nEpoch 4, Train Loss: 0.6959986346108573, Val Loss: 0.6533758640289307\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6821357096944537, Val Loss: 492.1303405761719\nEpoch 2, Train Loss: 30.664601343018667, Val Loss: 0.8114951848983765\nEpoch 3, Train Loss: 0.7764935408319745, Val Loss: 0.6871203184127808\nEpoch 4, Train Loss: 0.6905723128999982, Val Loss: 0.8845758438110352\nEpoch 5, Train Loss: 0.6918439183916364, Val Loss: 0.8174288272857666\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.6845783846718925, Val Loss: 3907.10400390625\nEpoch 2, Train Loss: 246.67246271882738, Val Loss: 0.7917137742042542\nEpoch 3, Train Loss: 0.8381595441273281, Val Loss: 0.5733122229576111\nEpoch 4, Train Loss: 2.659170082637242, Val Loss: 0.3571406602859497\nEpoch 5, Train Loss: 0.8310362696647644, Val Loss: 0.7074723839759827\nEpoch 6, Train Loss: 0.7014245305742536, Val Loss: 0.8534478545188904\nEarly stopping at epoch 6\nEpoch 1, Train Loss: 0.6789874051298413, Val Loss: 39.69042205810547\nEpoch 2, Train Loss: 4.582714659827096, Val Loss: 0.5668492913246155\nEpoch 3, Train Loss: 0.8065130455153329, Val Loss: 0.868307888507843\nEpoch 4, Train Loss: 0.692055344581604, Val Loss: 0.6573702096939087\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6977793404034206, Val Loss: 0.0006562151829712093\nEpoch 2, Train Loss: 2.316669021333967, Val Loss: 1.721243977546692\nEpoch 3, Train Loss: 0.797241577080318, Val Loss: 0.020333167165517807\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.649626727615084, Val Loss: 0.0002558571577537805\nEpoch 2, Train Loss: 1.4630528262683324, Val Loss: 0.90701824426651\nEpoch 3, Train Loss: 1.0765286598886763, Val Loss: 6.414780139923096\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6563698351383209, Val Loss: 0.00016027780657168478\nEpoch 2, Train Loss: 15.790240406990051, Val Loss: 0.2818417251110077\nEpoch 3, Train Loss: 0.8080413256372724, Val Loss: 0.38380706310272217\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6492596098354885, Val Loss: 7.143984794616699\nEpoch 2, Train Loss: 1.3699770825249808, Val Loss: 0.01157275028526783\nEpoch 3, Train Loss: 0.9667582767350333, Val Loss: 0.9749735593795776\nEpoch 4, Train Loss: 0.6966922453471592, Val Loss: 0.8470326662063599\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.6724695137568882, Val Loss: 0.0\nEpoch 2, Train Loss: 13.019839746611458, Val Loss: 0.14590387046337128\nEpoch 3, Train Loss: 0.8859124183654785, Val Loss: 1.1489429473876953\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6712395208222526, Val Loss: 8.22708971099928e-05\nEpoch 2, Train Loss: 4.362595924309322, Val Loss: 2.4782395362854004\nEpoch 3, Train Loss: 0.7672880632536752, Val Loss: 0.4429643154144287\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6932416217667716, Val Loss: 6.758150577545166\nEpoch 2, Train Loss: 18.754060872963496, Val Loss: 0.5410316586494446\nEpoch 3, Train Loss: 0.866141506603786, Val Loss: 1.9391266107559204\nEpoch 4, Train Loss: 0.7514055797031948, Val Loss: 0.548682689666748\nEarly stopping at epoch 4\nEpoch 1, Train Loss: 0.7273179633276803, Val Loss: 0.0\nEpoch 2, Train Loss: 37.823624065944124, Val Loss: 2.0672998428344727\nEpoch 3, Train Loss: 1.0699761850493295, Val Loss: 1.7102253437042236\nEarly stopping at epoch 3\nEpoch 1, Train Loss: 0.6625768116542271, Val Loss: 7.917964935302734\nEpoch 2, Train Loss: 526.6742864506585, Val Loss: 4.975620269775391\nEpoch 3, Train Loss: 1.1201360821723938, Val Loss: 0.8446090817451477\nEpoch 4, Train Loss: 0.7096866284097944, Val Loss: 0.8016542792320251\nEpoch 5, Train Loss: 0.6909145287105015, Val Loss: 0.8898155689239502\nEpoch 6, Train Loss: 0.6898659212248666, Val Loss: 0.7896580100059509\nEpoch 7, Train Loss: 0.6875103797231402, Val Loss: 0.8344917893409729\nEpoch 8, Train Loss: 0.6907868981361389, Val Loss: 0.7039488554000854\nEpoch 9, Train Loss: 0.6891074776649475, Val Loss: 0.766839325428009\nEpoch 10, Train Loss: 0.6884288787841797, Val Loss: 0.9215585589408875\nEarly stopping at epoch 10\nEpoch 1, Train Loss: 0.6132801686014447, Val Loss: 8.042023658752441\nEpoch 2, Train Loss: 5.264718149389539, Val Loss: 0.6006186008453369\nEpoch 3, Train Loss: 0.6997933217457363, Val Loss: 0.47939950227737427\nEpoch 4, Train Loss: 0.7062018258231026, Val Loss: 0.8336098194122314\nEpoch 5, Train Loss: 0.6890307239123753, Val Loss: 0.878829836845398\nEarly stopping at epoch 5\nEpoch 1, Train Loss: 0.6509135961532593, Val Loss: 0.0\nEpoch 2, Train Loss: 4.376505042825427, Val Loss: 0.7300960421562195\nEpoch 3, Train Loss: 1.008359738758632, Val Loss: 0.8649283051490784\nEarly stopping at epoch 3\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       0.0\n           1       0.00      0.00      0.00      28.0\n\n    accuracy                           0.00      28.0\n   macro avg       0.00      0.00      0.00      28.0\nweighted avg       0.00      0.00      0.00      28.0\n\n\n\ntotal_Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.50      0.61      0.55       935\n           1       0.36      0.26      0.30       773\n\n    accuracy                           0.45      1708\n   macro avg       0.43      0.43      0.42      1708\nweighted avg       0.43      0.45      0.44      1708\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(np.argmax(total_y_true, axis=1), total_y_pred)\nprint(\"total_Classification Report:\")\nprint(report)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:15:35.144872Z","iopub.execute_input":"2024-04-19T14:15:35.145581Z","iopub.status.idle":"2024-04-19T14:15:35.317278Z","shell.execute_reply.started":"2024-04-19T14:15:35.145547Z","shell.execute_reply":"2024-04-19T14:15:35.315926Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m----> 3\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_y_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, total_y_pred)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n","\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"],"ename":"AxisError","evalue":"axis 1 is out of bounds for array of dimension 1","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"import torchvision.models as models\n\nresnext = models.resnext101_32x8d(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:18:23.526784Z","iopub.execute_input":"2024-04-19T14:18:23.527082Z","iopub.status.idle":"2024-04-19T14:18:34.500830Z","shell.execute_reply.started":"2024-04-19T14:18:23.527058Z","shell.execute_reply":"2024-04-19T14:18:34.500035Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n100%|██████████| 340M/340M [00:06<00:00, 57.6MB/s] \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class ModifiedResNeXt(nn.Module):\n    def __init__(self, num_classes):\n        super(ModifiedResNeXt, self).__init__()\n        # Load the pre-trained ResNeXt model\n        resnext = models.resnext101_32x8d(pretrained=True)\n\n        # Modify the input layer\n        resnext.conv1 = nn.Conv2d(19, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n        # Modify the output layer\n        resnext.fc = nn.Linear(resnext.fc.in_features, num_classes)\n\n        # Freeze the parameters of the pre-trained layers\n        for param in resnext.parameters():\n            param.requires_grad = True\n\n        # Make the parameters of the modified layers trainable\n        for param in resnext.fc.parameters():\n            param.requires_grad = True\n\n        self.resnext = resnext\n\n    def forward(self, x):\n        return self.resnext(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:22:10.249094Z","iopub.execute_input":"2024-04-19T14:22:10.249857Z","iopub.status.idle":"2024-04-19T14:22:10.257137Z","shell.execute_reply.started":"2024-04-19T14:22:10.249827Z","shell.execute_reply":"2024-04-19T14:22:10.256155Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:23:59.415341Z","iopub.execute_input":"2024-04-19T14:23:59.416166Z","iopub.status.idle":"2024-04-19T14:23:59.420643Z","shell.execute_reply.started":"2024-04-19T14:23:59.416132Z","shell.execute_reply":"2024-04-19T14:23:59.419688Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.metrics import accuracy_score\n\n# Load your dataset\n\n# Assuming your dataset is in the form of a list of dictionaries\n# where each dictionary represents a subject with data, labels, and a group identifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 256\nlogo = LeaveOneGroupOut()\ngroup_indices = logo.split(np.arange(len(dataset_total)), groups=groups_total)\ntotal_y_true = []\ntotal_y_pred = []\n\nfor train_indices, test_indices in group_indices:\n    # Split the dataset into train and test sets based on group indices\n    train_data = [dataset_total[i] for i in train_indices]\n    test_data = [dataset_total[i] for i in test_indices]\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n    # Initialize the model and optimizer\n    model = ModifiedResNeXt(2).to(device)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Early stopping parameters\n    patience = 2# Number of epochs to wait before early stopping\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    best_model_state = None  # Variable to store the best model's state\n\n    # Training loop\n    num_epochs = 150\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs1, labels in train_loader:\n            optimizer.zero_grad()\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Calculate validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs1, labels in test_loader:\n                inputs1 = inputs1.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs1)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        val_loss /= len(test_loader)\n        print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}\")\n\n        # Early stopping and save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()  # Save the best model's state\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n\n        if early_stop_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    # Load the best model's state\n    model.load_state_dict(best_model_state)\n\n    # Evaluate on the test set\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs1, labels in test_loader:\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(predicted.cpu().tolist())\n\n    total_y_true.extend(labels.cpu().tolist())\n    total_y_pred.extend(predicted.cpu().tolist())\n\n# Calculate classification report\nreport = classification_report(np.argmax(y_true, axis=1), y_pred)\nprint(\"Classification Report:\")\nprint(report)\nprint(\"\\n\")\n\nreport = classification_report(np.argmax(total_y_true, axis=1), total_y_pred)\nprint(\"total_Classification Report:\")\nprint(report)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:24:06.801624Z","iopub.execute_input":"2024-04-19T14:24:06.801964Z","iopub.status.idle":"2024-04-19T14:24:08.830086Z","shell.execute_reply.started":"2024-04-19T14:24:06.801939Z","shell.execute_reply":"2024-04-19T14:24:08.828757Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize the model and optimizer\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModifiedResNeXt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer\u001b[39;00m\n\u001b[1;32m     30\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (1 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 34.12 MiB is free. Process 2451 has 15.86 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 888.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 34.12 MiB is free. Process 2451 has 15.86 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 888.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\n# Step 1: Define the modified Inception v3 model\nclass ModifiedInceptionV3(nn.Module):\n    def __init__(self, num_classes):\n        super(ModifiedInceptionV3, self).__init__()\n        # Load the pre-trained Inception v3 model\n        inception = models.inception_v3(pretrained=True)\n\n        # Modify the input layer\n        inception.Conv2d_1a_3x3.conv = nn.Conv2d(19, 32, kernel_size=3, stride=2, bias=False)\n\n        # Modify the output layer\n        num_ftrs = inception.fc.in_features\n        inception.fc = nn.Linear(num_ftrs, num_classes)\n\n        # Freeze the parameters of the pre-trained layers\n        for param in inception.parameters():\n            param.requires_grad = True\n\n        # Make the parameters of the modified layers trainable\n        for param in inception.fc.parameters():\n            param.requires_grad = True\n\n        self.inception = inception\n\n    def forward(self, x):\n        return self.inception(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:37:30.553018Z","iopub.execute_input":"2024-04-19T14:37:30.553396Z","iopub.status.idle":"2024-04-19T14:37:33.165286Z","shell.execute_reply.started":"2024-04-19T14:37:30.553368Z","shell.execute_reply":"2024-04-19T14:37:33.164486Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.metrics import accuracy_score\n\n# Load your dataset\n\n# Assuming your dataset is in the form of a list of dictionaries\n# where each dictionary represents a subject with data, labels, and a group identifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 256\nlogo = LeaveOneGroupOut()\ngroup_indices = logo.split(np.arange(len(dataset_total)), groups=groups_total)\ntotal_y_true = []\ntotal_y_pred = []\n\nfor train_indices, test_indices in group_indices:\n    # Split the dataset into train and test sets based on group indices\n    train_data = [dataset_total[i] for i in train_indices]\n    test_data = [dataset_total[i] for i in test_indices]\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n    # Initialize the model and optimizer\n    model = ModifiedInceptionV3(2).to(device)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Early stopping parameters\n    patience = 2# Number of epochs to wait before early stopping\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    best_model_state = None  # Variable to store the best model's state\n\n    # Training loop\n    num_epochs = 150\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs1, labels in train_loader:\n            optimizer.zero_grad()\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Calculate validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs1, labels in test_loader:\n                inputs1 = inputs1.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs1)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        val_loss /= len(test_loader)\n        print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}\")\n\n        # Early stopping and save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()  # Save the best model's state\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n\n        if early_stop_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    # Load the best model's state\n    model.load_state_dict(best_model_state)\n\n    # Evaluate on the test set\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs1, labels in test_loader:\n            inputs1 = inputs1.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs1)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(predicted.cpu().tolist())\n\n    total_y_true.extend(labels.cpu().tolist())\n    total_y_pred.extend(predicted.cpu().tolist())\n\n# Calculate classification report\nreport = classification_report(np.argmax(y_true, axis=1), y_pred)\nprint(\"Classification Report:\")\nprint(report)\nprint(\"\\n\")\n\nreport = classification_report(np.argmax(total_y_true, axis=1), total_y_pred)\nprint(\"total_Classification Report:\")\nprint(report)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:44:30.724192Z","iopub.execute_input":"2024-04-19T14:44:30.724578Z","iopub.status.idle":"2024-04-19T14:44:33.248599Z","shell.execute_reply.started":"2024-04-19T14:44:30.724548Z","shell.execute_reply":"2024-04-19T14:44:33.247287Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 149MB/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m inputs1 \u001b[38;5;241m=\u001b[39m inputs1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mModifiedInceptionV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:105\u001b[0m, in \u001b[0;36mInception3._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[Tensor]]:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# N x 3 x 299 x 299\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d_1a_3x3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# N x 32 x 149 x 149\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2d_2a_3x3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:405\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 19, 3, 3], expected input[256, 3, 27, 750] to have 19 channels, but got 3 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [32, 19, 3, 3], expected input[256, 3, 27, 750] to have 19 channels, but got 3 channels instead","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1735, n_features=20, n_classes=2, n_clusters_per_class=1, random_state=65)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a logistic regression classifier\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\n# Predict the labels for the test set\ny_pred = classifier.predict(X_test)\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred)\n\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:49:31.999227Z","iopub.execute_input":"2024-04-19T14:49:31.999632Z","iopub.status.idle":"2024-04-19T14:49:32.036833Z","shell.execute_reply.started":"2024-04-19T14:49:31.999601Z","shell.execute_reply":"2024-04-19T14:49:32.035609Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99       261\n           1       1.00      0.98      0.99       260\n\n    accuracy                           0.99       521\n   macro avg       0.99      0.99      0.99       521\nweighted avg       0.99      0.99      0.99       521\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate some example data\ny_true = [0] * 948 + [1] * 787  # 0 represents class 1, 1 represents class 2\ny_pred = [0] * 250 + [1] * 1485  # Predicting class 1 for 200 points, class 2 for 1535 points\n\n# Print the classification report\nprint(classification_report(y_true, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:54:45.287327Z","iopub.execute_input":"2024-04-19T14:54:45.287695Z","iopub.status.idle":"2024-04-19T14:54:45.304374Z","shell.execute_reply.started":"2024-04-19T14:54:45.287665Z","shell.execute_reply":"2024-04-19T14:54:45.303383Z"},"trusted":true},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      0.26      0.42       948\n           1       0.53      1.00      0.69       787\n\n    accuracy                           0.60      1735\n   macro avg       0.76      0.63      0.56      1735\nweighted avg       0.79      0.60      0.54      1735\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}