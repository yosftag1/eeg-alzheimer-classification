{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for Graph Neural Networks\n",
    "\n",
    "This notebook performs systematic hyperparameter optimization for graph-based EEG classification models.\n",
    "\n",
    "## Optimization Strategy\n",
    "\n",
    "### 1. Graph Construction Parameters\n",
    "- **Connectivity Thresholds**: Optimize edge pruning thresholds\n",
    "- **Node Features**: Select optimal feature representations\n",
    "- **Graph Pooling**: Compare different pooling strategies\n",
    "\n",
    "### 2. Model Architecture Parameters\n",
    "- **Hidden Layer Dimensions**: Optimize GNN layer sizes\n",
    "- **Number of Layers**: Find optimal network depth\n",
    "- **Activation Functions**: Compare different activations\n",
    "- **Dropout Rates**: Optimize regularization strength\n",
    "\n",
    "### 3. Training Parameters\n",
    "- **Learning Rate**: Adaptive learning rate schedules\n",
    "- **Batch Size**: Memory-performance trade-offs\n",
    "- **Optimizer Settings**: Adam vs SGD parameter tuning\n",
    "- **Weight Decay**: L2 regularization optimization\n",
    "\n",
    "## Search Methods\n",
    "- **Grid Search**: Systematic parameter exploration\n",
    "- **Random Search**: Efficient sampling strategy\n",
    "- **Bayesian Optimization**: Advanced parameter search\n",
    "- **Cross-Validation**: Robust performance estimation\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **Accuracy**: Overall classification performance\n",
    "- **F1-Score**: Balanced precision-recall measure\n",
    "- **ROC-AUC**: Discriminative ability assessment\n",
    "- **Training Efficiency**: Computational cost analysis\n",
    "\n",
    "## Output\n",
    "- Optimal hyperparameter configurations\n",
    "- Performance comparison across parameter settings\n",
    "- Statistical significance analysis\n",
    "- Best model recommendations for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:40:46.410784Z",
     "iopub.status.busy": "2024-06-20T18:40:46.409920Z",
     "iopub.status.idle": "2024-06-20T18:41:00.767215Z",
     "shell.execute_reply": "2024-06-20T18:41:00.766045Z",
     "shell.execute_reply.started": "2024-06-20T18:40:46.410749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages for graph neural networks\n",
    "!pip install networkx torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:00.769478Z",
     "iopub.status.busy": "2024-06-20T18:41:00.769168Z",
     "iopub.status.idle": "2024-06-20T18:41:07.427069Z",
     "shell.execute_reply": "2024-06-20T18:41:07.426242Z",
     "shell.execute_reply.started": "2024-06-20T18:41:00.769449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:07.428986Z",
     "iopub.status.busy": "2024-06-20T18:41:07.428323Z",
     "iopub.status.idle": "2024-06-20T18:41:07.549250Z",
     "shell.execute_reply": "2024-06-20T18:41:07.548364Z",
     "shell.execute_reply.started": "2024-06-20T18:41:07.428948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, let's identify the unique groups in the DataFrame\n",
    "groups = subjects_info['Group'].unique()\n",
    "groups=[\"A\",\"C\"]\n",
    "# Now, let's split each group individually\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "total=[]\n",
    "for group in groups:\n",
    "    # Filter the DataFrame for the current group\n",
    "    group_df = subjects_info[subjects_info['Group'] == group]\n",
    "    \n",
    "    # Split the group data into training and testing sets while maintaining balance in gender\n",
    "    train_group, test_group = train_test_split(group_df, test_size=0.3, stratify=group_df['Gender'], random_state=42)\n",
    "    total.append(group_df)\n",
    "    # Append the split data to the lists\n",
    "    train_dfs.append(train_group)\n",
    "    test_dfs.append(test_group)\n",
    "\n",
    "# Concatenate the training and testing DataFrames for all groups\n",
    "train_df = pd.concat(train_dfs)\n",
    "test_df = pd.concat(test_dfs)\n",
    "total_df=pd.concat(total)\n",
    "# Now, train_df and test_df contain the split data with balanced groups and secondary balance in gender\n",
    "# Extracting subject IDs from the training set\n",
    "training_subjects = train_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "\n",
    "# Extracting subject IDs from the testing set\n",
    "testing_subjects = test_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "\n",
    "total_subjects = total_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "\n",
    "# Displaying the lists of subjects\n",
    "print(\"Training Subjects:\")\n",
    "print(training_subjects)\n",
    "\n",
    "print(\"\\nTesting Subjects:\")\n",
    "print(testing_subjects)\n",
    "\n",
    "print(\"\\nTotal Subjects:\")\n",
    "print(total_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:07.552121Z",
     "iopub.status.busy": "2024-06-20T18:41:07.551754Z",
     "iopub.status.idle": "2024-06-20T18:41:07.565984Z",
     "shell.execute_reply": "2024-06-20T18:41:07.565060Z",
     "shell.execute_reply.started": "2024-06-20T18:41:07.552085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory paths\n",
    "pairwise_features_dir = '/kaggle/input/new-extraction/eeg_connectivity_features/'\n",
    "node_features_dir = '/kaggle/input/new-extraction/node_features/'\n",
    "\n",
    "# Function to load data\n",
    "def load_data(subject_id):\n",
    "    pairwise_path = os.path.join(pairwise_features_dir, f'eeg_connectivity_features_subject_{subject_id}.csv')\n",
    "    node_path = os.path.join(node_features_dir, f'node_features_subject_{subject_id}.csv')\n",
    "    \n",
    "    pairwise_df = pd.read_csv(pairwise_path)\n",
    "    node_df = pd.read_csv(node_path)\n",
    "    \n",
    "    return pairwise_df, node_df\n",
    "\n",
    "# Function to create graph from data\n",
    "def create_graph(pairwise_df, node_df, sample):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with attributes from node_df\n",
    "    for _, row in node_df[node_df['sample'] == sample].iterrows():\n",
    "        G.add_node(row['channel'], **{\n",
    "            'psd_mean': row['psd_mean'],\n",
    "            'psd_std': row['psd_std'],\n",
    "            'entropy': row['entropy'],\n",
    "            'hjorth_activity': row['hjorth_activity'],\n",
    "            'hjorth_mobility': row['hjorth_mobility'],\n",
    "            'hjorth_complexity': row['hjorth_complexity'],\n",
    "            'spectral_entropy': row['spectral_entropy']\n",
    "        })\n",
    "    \n",
    "    # Add edges with attributes from pairwise_df\n",
    "    for _, row in pairwise_df[pairwise_df['sample'] == sample].iterrows():\n",
    "        G.add_edge(row['channel_1'], row['channel_2'], **{\n",
    "            \n",
    "            'plv': row['plv'],\n",
    "            'mutual_information': row['mutual_information'],\n",
    "            'pearson_correlation': row['pearson_correlation']\n",
    "        })\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Process subjects\n",
    "\n",
    "# Optionally save graphs or process them for a graph neural network framework\n",
    "\n",
    "# Example: Converting to PyTorch Geometric format\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "def nx_to_pyg_data(G,label):\n",
    "    # Convert NetworkX graph to PyTorch Geometric Data\n",
    "    edge_index = torch.tensor(list(G.edges)).t().contiguous().long()\n",
    "    \n",
    "    # Node features\n",
    "    x = []\n",
    "    for _, data in G.nodes(data=True):\n",
    "        x.append([data['psd_mean'], data['psd_std'], data['entropy'], data['hjorth_activity'],\n",
    "                  data['hjorth_mobility'], data['hjorth_complexity'], data['spectral_entropy']])\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    # Edge features\n",
    "    edge_attr = []\n",
    "    for _, _, data in G.edges(data=True):\n",
    "        edge_attr.append([ data['plv'], data['mutual_information'], data['pearson_correlation']])\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=torch.tensor([label], dtype=torch.float))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:07.567439Z",
     "iopub.status.busy": "2024-06-20T18:41:07.567108Z",
     "iopub.status.idle": "2024-06-20T18:41:43.153130Z",
     "shell.execute_reply": "2024-06-20T18:41:43.152209Z",
     "shell.execute_reply.started": "2024-06-20T18:41:07.567407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_graphs = []\n",
    "groups = []\n",
    "count = 0\n",
    "subjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')\n",
    "labels = []\n",
    "\n",
    "for subject_id in tqdm(total_subjects):\n",
    "    pairwise_df, node_df = load_data(subject_id)\n",
    "    samples = pairwise_df['sample'].unique()\n",
    "    count += len(samples)\n",
    "    num_epochs = len(samples)  # Number of epochs, replace 'your_data_key' as before\n",
    "    \n",
    "    subject_id = f\"sub-{str(subject_id).zfill(3)}\"  # Format subject ID\n",
    "    group_info = subjects_info[subjects_info['participant_id'] == subject_id]['Group'].values[0]\n",
    "    duplicated_groups = [group_info] * num_epochs\n",
    "    labels.extend(duplicated_groups)\n",
    "\n",
    "    groups.extend([subject_id] * num_epochs)\n",
    "    \n",
    "    for sample in samples:\n",
    "        G = create_graph(pairwise_df, node_df, sample)\n",
    "        all_graphs.append(G)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:43.154609Z",
     "iopub.status.busy": "2024-06-20T18:41:43.154305Z",
     "iopub.status.idle": "2024-06-20T18:41:43.173032Z",
     "shell.execute_reply": "2024-06-20T18:41:43.172092Z",
     "shell.execute_reply.started": "2024-06-20T18:41:43.154584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming you define label_mapping, encoder, and encode labels as in your code\n",
    "#####################################################\n",
    "label_mapping = {'A': 0, 'C': 1, 'F': 2}  # You can adjust this mapping as needed\n",
    "#####################################################\n",
    "df=pd.Series(labels).map(label_mapping)\n",
    "\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "encoder.fit(np.array(pd.Series(labels).map(label_mapping)).reshape(-1, 1))\n",
    "# Encode labels\n",
    "################################################\n",
    "\n",
    "total_labels_encoded = encoder.transform(np.array(pd.Series(labels).map(label_mapping).values).reshape(-1, 1))\n",
    "################################################\n",
    "\n",
    "\n",
    "print(\"Total labels shape:\", total_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:43.174662Z",
     "iopub.status.busy": "2024-06-20T18:41:43.174262Z",
     "iopub.status.idle": "2024-06-20T18:41:44.451177Z",
     "shell.execute_reply": "2024-06-20T18:41:44.450200Z",
     "shell.execute_reply.started": "2024-06-20T18:41:43.174614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pyg_graphs = [nx_to_pyg_data(G,labels) for G, labels in zip(all_graphs, total_labels_encoded)]\n",
    "print(len(pyg_graphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:44.452646Z",
     "iopub.status.busy": "2024-06-20T18:41:44.452334Z",
     "iopub.status.idle": "2024-06-20T18:41:44.461903Z",
     "shell.execute_reply": "2024-06-20T18:41:44.461032Z",
     "shell.execute_reply.started": "2024-06-20T18:41:44.452609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU\n",
    "from torch_geometric.nn import NNConv, global_mean_pool\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        # Define the network for edge features\n",
    "        nn = Sequential(\n",
    "            Linear(edge_in_channels, 25),\n",
    "            ReLU(),\n",
    "            Linear(25, in_channels * hidden_channels)\n",
    "        )\n",
    "        self.conv1 = NNConv(in_channels, hidden_channels, nn, aggr='mean')\n",
    "        \n",
    "        # Define another network for the second NNConv layer if needed\n",
    "        nn2 = Sequential(\n",
    "            Linear(edge_in_channels, 25),\n",
    "            ReLU(),\n",
    "            Linear(25, hidden_channels * hidden_channels)\n",
    "        )\n",
    "        self.conv2 = NNConv(hidden_channels, hidden_channels, nn2, aggr='mean')\n",
    "        \n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = global_mean_pool(x, batch)  # Global pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:41:44.463422Z",
     "iopub.status.busy": "2024-06-20T18:41:44.463129Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "patience = 50  # Number of epochs to wait before early stopping\n",
    "labels_total = np.array(total_labels_encoded)  # Convert to numpy array\n",
    "binary_labels = np.any(labels_total, axis=1).astype(int)  # Convert to binary (0 or 1)\n",
    "\n",
    "def objective(trial):\n",
    "    fold = 1\n",
    "    group_indices = skf.split(np.arange(len(pyg_graphs)), groups)\n",
    "\n",
    "    total_y_true = []\n",
    "    total_y_pred = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 16, 128)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    \n",
    "    for train_indices, test_indices in group_indices:\n",
    "        print(f\"Fold {fold}/5\")\n",
    "        \n",
    "        # Split the dataset into train and test sets based on indices\n",
    "        train_data = [pyg_graphs[i] for i in train_indices]\n",
    "        test_data = [pyg_graphs[i] for i in test_indices]\n",
    "        \n",
    "        # Create DataLoader for train and test sets\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = GNN(in_channels=7, edge_in_channels=3, hidden_channels=hidden_channels, out_channels=2).to(device)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        best_model_state = None  # Variable to store the best model's state\n",
    "\n",
    "        # Training loop\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                loss = criterion(out, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                    loss = criterion(out, batch.y)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(test_loader) + 0.001\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}\")\n",
    "\n",
    "            train_loss_list.append(running_loss/len(train_loader))\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "            # Early stopping and save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()  # Save the best model's state\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Load the best model's state\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                _, predicted = torch.max(out, 1)\n",
    "                y_true.extend(batch.y.cpu().tolist())\n",
    "                y_pred.extend(predicted.cpu().tolist())\n",
    "                total_y_true.extend(batch.y.cpu().tolist())\n",
    "                total_y_pred.extend(predicted.cpu().tolist())\n",
    "\n",
    "        # Calculate classification metrics\n",
    "        acc = accuracy_score(np.argmax(y_true, axis=1), y_pred)\n",
    "        sens = recall_score(np.argmax(y_true, axis=1), y_pred, average='binary', pos_label=1)\n",
    "        spec = recall_score(np.argmax(y_true, axis=1), y_pred, average='binary', pos_label=0)\n",
    "        prec = precision_score(np.argmax(y_true, axis=1), y_pred, average='binary', pos_label=1)\n",
    "        f1 = f1_score(np.argmax(y_true, axis=1), y_pred, average='binary', pos_label=1)\n",
    "        recall = recall_score(np.argmax(y_true, axis=1), y_pred, average='binary', pos_label=1)\n",
    "        confusion_mat = confusion_matrix(np.argmax(y_true, axis=1), y_pred)\n",
    "\n",
    "        print(f\"Fold {fold}/5 Classification Metrics:\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Sensitivity: {sens:.4f}\")\n",
    "        print(f\"Specificity: {spec:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_mat}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Plot training curves for the current fold\n",
    "        plt.figure()\n",
    "        plt.plot(range(len(train_loss_list)), train_loss_list, label='Train Loss')\n",
    "        plt.plot(range(len(val_loss_list)), val_loss_list, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training Curves for Fold {fold}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        train_losses.extend(train_loss_list)\n",
    "        val_losses.extend(val_loss_list)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(f\"Best trial:\\n{study.best_trial}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4366744,
     "sourceId": 7499077,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 172069479,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 184060495,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
