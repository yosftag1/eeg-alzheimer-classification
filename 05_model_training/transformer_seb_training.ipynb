{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with Squeeze-and-Excitation Training\n",
    "\n",
    "This notebook implements a dual-input neural network that combines:\n",
    "- **Convolutional Neural Networks** with Squeeze-and-Excitation (SE) blocks for feature extraction\n",
    "- **Transformer encoders** for sequential pattern learning\n",
    "- **Cross-validation** for robust performance evaluation\n",
    "\n",
    "## Model Architecture\n",
    "- **Dual Input Processing**: Handles coherence/SCC data and RBP features separately\n",
    "- **SE Blocks**: Channel attention mechanisms for improved feature learning\n",
    "- **Transformer Layers**: Self-attention for capturing long-range dependencies\n",
    "- **Multi-modal Fusion**: Combines features from both input streams\n",
    "\n",
    "## Key Features\n",
    "- 10-fold stratified cross-validation\n",
    "- Early stopping to prevent overfitting\n",
    "- Comprehensive performance metrics\n",
    "- Training curve visualization\n",
    "- Statistical analysis across folds\n",
    "\n",
    "## Expected Performance\n",
    "This model showed good performance in initial experiments with:\n",
    "- High accuracy on Alzheimer's vs Control classification\n",
    "- Robust cross-validation results\n",
    "- Effective fusion of multiple EEG feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6d04358-629d-4da4-8bf1-322ff7399bf1",
    "_uuid": "8ee7fb71-8acc-4567-89ca-306820830f02",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T16:47:05.313880Z",
     "iopub.status.busy": "2024-06-30T16:47:05.313499Z",
     "iopub.status.idle": "2024-06-30T16:47:05.319881Z",
     "shell.execute_reply": "2024-06-30T16:47:05.318987Z",
     "shell.execute_reply.started": "2024-06-30T16:47:05.313849Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Neural network components\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "edf080af-9726-4c8d-9884-518fa0dbd369",
    "_uuid": "7e2f581f-2f17-4133-8e87-a3d22306a573",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:30.627029Z",
     "iopub.status.busy": "2024-06-30T17:27:30.626364Z",
     "iopub.status.idle": "2024-06-30T17:27:30.655287Z",
     "shell.execute_reply": "2024-06-30T17:27:30.654361Z",
     "shell.execute_reply.started": "2024-06-30T17:27:30.626997Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load participant information and create train/test splits\n",
    "subjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Focus on Alzheimer's (A) and Control (C) groups\n",
    "groups = [\"A\", \"C\"]\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "total = []\n",
    "\n",
    "# Split each group while maintaining gender balance\n",
    "for group in groups:\n",
    "    group_df = subjects_info[subjects_info['Group'] == group]\n",
    "    train_group, test_group = train_test_split(\n",
    "        group_df, test_size=0.3, stratify=group_df['Gender'], random_state=42\n",
    "    )\n",
    "    total.append(group_df)\n",
    "    train_dfs.append(train_group)\n",
    "    test_dfs.append(test_group)\n",
    "\n",
    "# Combine all splits\n",
    "train_df = pd.concat(train_dfs)\n",
    "test_df = pd.concat(test_dfs)\n",
    "total_df = pd.concat(total)\n",
    "\n",
    "# Extract subject IDs\n",
    "training_subjects = train_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "testing_subjects = test_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "total_subjects = total_df['participant_id'].str.extract(r'sub-(\\d+)').astype(int).squeeze().unique().tolist()\n",
    "\n",
    "print(\"Training Subjects:\", training_subjects)\n",
    "print(\"Testing Subjects:\", testing_subjects)\n",
    "print(\"Total Subjects:\", total_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "21b0b3a3-b46d-49b5-b6f3-ac11fbd2b887",
    "_uuid": "2fa06722-7088-4a4e-8c37-7b19265e53a0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:32.252440Z",
     "iopub.status.busy": "2024-06-30T17:27:32.251771Z",
     "iopub.status.idle": "2024-06-30T17:27:32.262436Z",
     "shell.execute_reply": "2024-06-30T17:27:32.261449Z",
     "shell.execute_reply.started": "2024-06-30T17:27:32.252387Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(subjects_info, subjects, data_type='training'):\n",
    "    \"\"\"\n",
    "    Load EEG data for specified subjects including coherence, RBP, and SCC features.\n",
    "    \n",
    "    Args:\n",
    "        subjects_info: DataFrame containing subject information\n",
    "        subjects: List of subject IDs to load\n",
    "        data_type: Type of data loading (for logging purposes)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (coherences, rbps, scc, labels, groups)\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    coherences = []\n",
    "    rbps = []\n",
    "    scc = []\n",
    "    labels = []\n",
    "    \n",
    "    output_folder = \"/kaggle/input/fork-of-extraction-cleaned\"\n",
    "    \n",
    "    for idx in subjects:\n",
    "        # Define file paths for different feature types\n",
    "        file_path_rbp = os.path.join(output_folder, f'rbp/rbp_{idx}.npy')\n",
    "        file_path_coherence = os.path.join(output_folder, f'coherence/coherence_{idx}.npy')\n",
    "        file_path_scc = os.path.join(output_folder, f'scc_cleaned_base/sub-{idx}_epochs.npy')\n",
    "        \n",
    "        # Load feature data\n",
    "        subject_data_coherence = np.load(file_path_coherence)\n",
    "        coherences.append(subject_data_coherence)\n",
    "        \n",
    "        subject_rbp = np.load(file_path_rbp)\n",
    "        rbps.append(subject_rbp)\n",
    "        \n",
    "        subject_scc = np.load(file_path_scc)\n",
    "        scc.append(subject_scc)\n",
    "        \n",
    "        # Create labels for each epoch\n",
    "        num_epochs = subject_rbp.shape[0]\n",
    "        subject_id = f\"sub-{str(idx).zfill(3)}\"\n",
    "        group_info = subjects_info[subjects_info['participant_id'] == subject_id]['Group'].values[0]\n",
    "        \n",
    "        # Extend labels and groups for all epochs of this subject\n",
    "        labels.extend([group_info] * num_epochs)\n",
    "        groups.extend([idx] * num_epochs)\n",
    "        \n",
    "    return (np.concatenate(coherences, axis=0), \n",
    "            np.concatenate(rbps, axis=0),\n",
    "            np.concatenate(scc, axis=0),\n",
    "            labels, \n",
    "            groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00201ef6-89b2-4d1c-b9ca-69ca1a2414c5",
    "_uuid": "0e919115-ea3e-4642-9050-f41ed002e80a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:34.098047Z",
     "iopub.status.busy": "2024-06-30T17:27:34.097232Z",
     "iopub.status.idle": "2024-06-30T17:27:34.104977Z",
     "shell.execute_reply": "2024-06-30T17:27:34.103975Z",
     "shell.execute_reply.started": "2024-06-30T17:27:34.098003Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load subject information\n",
    "subjects_info = pd.read_csv('/kaggle/input/open-nuro-dataset/dataset/participants.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d931f38-02f6-4b36-b943-da307fc2f635",
    "_uuid": "40f0f22f-e7b1-484f-86ac-05bcfe34d0f5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:34.336954Z",
     "iopub.status.busy": "2024-06-30T17:27:34.336593Z",
     "iopub.status.idle": "2024-06-30T17:27:35.086662Z",
     "shell.execute_reply": "2024-06-30T17:27:35.085848Z",
     "shell.execute_reply.started": "2024-06-30T17:27:34.336923Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load all data for training\n",
    "coherences_total, rbps_total, scc_total, total_labels, groups_total = load_data(\n",
    "    subjects_info, total_subjects, data_type='total'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f024483-9a48-4592-b19c-7cc6b6d6159f",
    "_uuid": "a94e3938-fd74-466f-be5b-e4e7929f59a9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:35.088462Z",
     "iopub.status.busy": "2024-06-30T17:27:35.088162Z",
     "iopub.status.idle": "2024-06-30T17:27:35.094273Z",
     "shell.execute_reply": "2024-06-30T17:27:35.093345Z",
     "shell.execute_reply.started": "2024-06-30T17:27:35.088437Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check data dimensions\n",
    "print(\"Coherences shape:\", coherences_total.shape)\n",
    "print(\"RBP shape:\", rbps_total.shape)\n",
    "print(\"SCC shape:\", scc_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8986771b-b4d5-4e30-aa90-3816f5a55d4c",
    "_uuid": "af2dca81-a606-48f7-8aa2-76ebfe2368b8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:35.462809Z",
     "iopub.status.busy": "2024-06-30T17:27:35.462108Z",
     "iopub.status.idle": "2024-06-30T17:27:35.474694Z",
     "shell.execute_reply": "2024-06-30T17:27:35.473700Z",
     "shell.execute_reply.started": "2024-06-30T17:27:35.462775Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define label mapping and encode labels\n",
    "label_mapping = {'A': 0, 'C': 1, 'F': 2}  # Alzheimer's, Control, Frontotemporal\n",
    "\n",
    "# Map string labels to numeric\n",
    "numeric_labels = pd.Series(total_labels).map(label_mapping)\n",
    "\n",
    "# Create and fit one-hot encoder\n",
    "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "encoder.fit(numeric_labels.values.reshape(-1, 1))\n",
    "\n",
    "# Encode labels to one-hot format\n",
    "total_labels_encoded = encoder.transform(numeric_labels.values.reshape(-1, 1))\n",
    "\n",
    "print(\"Total labels shape:\", total_labels_encoded.shape)\n",
    "print(\"Label distribution:\", pd.Series(total_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b24c31e-e2ce-4b80-a22f-ba045278d1e9",
    "_uuid": "e1918fb5-2c60-4265-9bbd-08310a8e01fd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:36.327857Z",
     "iopub.status.busy": "2024-06-30T17:27:36.327152Z",
     "iopub.status.idle": "2024-06-30T17:27:36.335223Z",
     "shell.execute_reply": "2024-06-30T17:27:36.334243Z",
     "shell.execute_reply.started": "2024-06-30T17:27:36.327827Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "coherence_tensor_total = torch.tensor(scc_total, dtype=torch.float32).unsqueeze(1)\n",
    "rbps_tensor_total = torch.tensor(rbps_total, dtype=torch.float32).unsqueeze(1)\n",
    "labels_tensor_total = torch.tensor(total_labels_encoded, dtype=torch.float32)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset_total = TensorDataset(coherence_tensor_total, rbps_tensor_total, labels_tensor_total)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset_total, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9be536f0-6ad2-457f-869d-5578d71f4df2",
    "_uuid": "54922b6b-3038-41fa-b4f8-29440c8de943",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T17:27:41.282521Z",
     "iopub.status.busy": "2024-06-30T17:27:41.282165Z",
     "iopub.status.idle": "2024-06-30T17:27:41.307312Z",
     "shell.execute_reply": "2024-06-30T17:27:41.306172Z",
     "shell.execute_reply.started": "2024-06-30T17:27:41.282492Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block for channel attention\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, reduction_ratio=8):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        out = self.squeeze(x).view(batch_size, channels)\n",
    "        out = self.excitation(out).view(batch_size, channels, 1, 1)\n",
    "        return x * out.expand_as(x)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-input CNN with Squeeze-and-Excitation blocks and Transformer layers.\n",
    "    Processes two input modalities: coherence data and RBP features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.15, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # First input path (coherence/SCC data)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.se1 = SEBlock(64)\n",
    "        self.conv2 = nn.Conv2d(64, 16, kernel_size=3, padding=1)\n",
    "        self.se2 = SEBlock(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 42, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Transformer for first path\n",
    "        self.transformer_layer1 = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(self.transformer_layer1, num_layers=2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Second input path (RBP features)\n",
    "        self.conv3 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.se3 = SEBlock(32)\n",
    "        self.conv4 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.se4 = SEBlock(16)\n",
    "        self.fc3 = nn.Linear(16 * 4, 128)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Transformer for second path\n",
    "        self.transformer_layer2 = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(self.transformer_layer2, num_layers=1)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Process first input (coherence/SCC)\n",
    "        x1 = self.pool(F.relu(self.se1(self.conv1(x1))))\n",
    "        x1 = self.pool(F.relu(self.se2(self.conv2(x1))))\n",
    "        x1 = x1.view(-1, 16 * 42)\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.dropout1(x1)\n",
    "\n",
    "        # Add positional encoding and apply transformer\n",
    "        seq_len = x1.size(0)\n",
    "        pos_encoding = self.get_positional_encoding(seq_len, 128).to(x1.device)\n",
    "        x1 = x1 + pos_encoding\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        x1 = self.transformer_encoder1(x1)\n",
    "        x1 = self.dropout2(x1.squeeze(1))\n",
    "\n",
    "        # Process second input (RBP)\n",
    "        x2 = self.pool(F.relu(self.se3(self.conv3(x2))))\n",
    "        x2 = self.pool(F.relu(self.se4(self.conv4(x2))))\n",
    "        x2 = x2.view(-1, 16 * 4)\n",
    "        x2 = F.relu(self.fc3(x2))\n",
    "        x2 = self.dropout3(x2)\n",
    "\n",
    "        # Add positional encoding and apply transformer\n",
    "        seq_len = x2.size(0)\n",
    "        pos_encoding = self.get_positional_encoding(seq_len, 128).to(x2.device)\n",
    "        x2 = x2 + pos_encoding\n",
    "        x2 = x2.unsqueeze(1)\n",
    "        x2 = self.transformer_encoder2(x2)\n",
    "        x2 = self.dropout4(x2.squeeze(1))\n",
    "\n",
    "        # Combine features and classify\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d_model):\n",
    "        \"\"\"Generate sinusoidal positional encoding\"\"\"\n",
    "        pos_encoding = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83933772-6589-46e6-b3f0-d477c9aefa57",
    "_uuid": "f507a73c-6edc-48c5-befb-34b19ca0acee",
    "execution": {
     "iopub.execute_input": "2024-06-30T17:31:28.259426Z",
     "iopub.status.busy": "2024-06-30T17:31:28.259011Z",
     "iopub.status.idle": "2024-06-30T17:33:55.607219Z",
     "shell.execute_reply": "2024-06-30T17:33:55.606146Z",
     "shell.execute_reply.started": "2024-06-30T17:31:28.259382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "inner_skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert multi-class labels to binary for stratification\n",
    "binary_labels = np.any(total_labels_encoded, axis=1).astype(int)\n",
    "group_indices = skf.split(np.arange(len(dataset_total)), groups_total)\n",
    "\n",
    "# Training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "num_epochs = 60\n",
    "patience = 25  # Early stopping patience\n",
    "\n",
    "# Storage for results\n",
    "total_y_true = []\n",
    "total_y_pred = []\n",
    "fold_metrics = []\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Starting {skf.n_splits}-fold cross-validation\")\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(group_indices, 1):\n",
    "    print(f\"\\n=== Fold {fold}/{skf.n_splits} ===\")\n",
    "    \n",
    "    # Create data splits\n",
    "    train_data = [dataset_total[i] for i in train_indices]\n",
    "    test_data = [dataset_total[i] for i in test_indices]\n",
    "    \n",
    "    # Inner split for validation\n",
    "    train_split_indices, val_split_indices = next(inner_skf.split(\n",
    "        np.arange(len(train_data)), \n",
    "        [groups_total[i] for i in train_indices]\n",
    "    ))\n",
    "    train_subset = [train_data[i] for i in train_split_indices]\n",
    "    val_subset = [train_data[i] for i in val_split_indices]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = ConvNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    best_model_state = None\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs1, inputs2, labels in train_loader:\n",
    "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs1, inputs2, labels in val_loader:\n",
    "                inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "                outputs = model(inputs1, inputs2)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs1, inputs2, labels in test_loader:\n",
    "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "            outputs = model(inputs1, inputs2)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    acc = accuracy_score(y_true_labels, y_pred)\n",
    "    sens = recall_score(y_true_labels, y_pred, average='macro')\n",
    "    spec = recall_score(y_true_labels, y_pred, average='macro', pos_label=0)\n",
    "    prec = precision_score(y_true_labels, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true_labels, y_pred, average='macro')\n",
    "    confusion_mat = confusion_matrix(y_true_labels, y_pred)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_metrics.append({\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Sensitivity\": sens,\n",
    "        \"Specificity\": spec,\n",
    "        \"Precision\": prec,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Recall\": sens,\n",
    "        \"Confusion Matrix\": confusion_mat\n",
    "    })\n",
    "    \n",
    "    # Add to total results\n",
    "    total_y_true.extend(y_true)\n",
    "    total_y_pred.extend(y_pred)\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"  Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {sens:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_loss_list, label='Train Loss', color='blue')\n",
    "    plt.plot(val_loss_list, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Curves - Fold {fold}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_y_true_labels = np.argmax(total_y_true, axis=1)\n",
    "overall_acc = accuracy_score(total_y_true_labels, total_y_pred)\n",
    "overall_prec = precision_score(total_y_true_labels, total_y_pred, average='macro')\n",
    "overall_rec = recall_score(total_y_true_labels, total_y_pred, average='macro')\n",
    "overall_f1 = f1_score(total_y_true_labels, total_y_pred, average='macro')\n",
    "overall_confusion = confusion_matrix(total_y_true_labels, total_y_pred)\n",
    "\n",
    "print(f\"\\n=== Overall Results Across All Folds ===\")\n",
    "print(f\"Accuracy: {overall_acc:.4f}\")\n",
    "print(f\"Precision: {overall_prec:.4f}\")\n",
    "print(f\"Recall: {overall_rec:.4f}\")\n",
    "print(f\"F1 Score: {overall_f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{overall_confusion}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "report = classification_report(total_y_true_labels, total_y_pred)\n",
    "print(f\"\\nDetailed Classification Report:\\n{report}\")\n",
    "\n",
    "# Calculate statistics across folds\n",
    "metrics_array = np.array([[m['Accuracy'], m['Sensitivity'], m['Specificity'], \n",
    "                          m['Precision'], m['F1 Score'], m['Recall']] for m in fold_metrics])\n",
    "metrics_mean = np.mean(metrics_array, axis=0)\n",
    "metrics_std = np.std(metrics_array, axis=0)\n",
    "metric_names = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'Recall']\n",
    "\n",
    "print(f\"\\n=== Cross-Validation Statistics ===\")\n",
    "print(f\"{'Metric':<12} {'Mean ± STD':<15}\")\n",
    "print(\"-\" * 27)\n",
    "for name, mean_val, std_val in zip(metric_names, metrics_mean, metrics_std):\n",
    "    print(f\"{name:<12} {mean_val:.4f} ± {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f333402c-45e5-48a5-a974-2661fdc23f5c",
    "_uuid": "adcbeb94-820c-4766-b4cf-6034decdf598",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-30T16:40:11.676480Z",
     "iopub.status.busy": "2024-06-30T16:40:11.675644Z",
     "iopub.status.idle": "2024-06-30T16:40:11.944921Z",
     "shell.execute_reply": "2024-06-30T16:40:11.943688Z",
     "shell.execute_reply.started": "2024-06-30T16:40:11.676437Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(overall_confusion, \n",
    "            annot=True, \n",
    "            cmap=\"Blues\", \n",
    "            fmt=\"d\", \n",
    "            xticklabels=['Alzheimer\\'s', 'Control'], \n",
    "            yticklabels=['Alzheimer\\'s', 'Control'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title(\"Overall Confusion Matrix\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Predicted Labels\", fontsize=12)\n",
    "plt.ylabel(\"True Labels\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional performance visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Metrics across folds\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'F1 Score', 'Recall']\n",
    "metrics_df[metrics_to_plot].boxplot(ax=ax1)\n",
    "ax1.set_title('Performance Metrics Distribution Across Folds')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Fold-wise accuracy\n",
    "ax2.plot(range(1, len(fold_metrics) + 1), [m['Accuracy'] for m in fold_metrics], \n",
    "         marker='o', linewidth=2, markersize=6)\n",
    "ax2.axhline(y=overall_acc, color='red', linestyle='--', label=f'Overall Mean: {overall_acc:.3f}')\n",
    "ax2.set_title('Accuracy Across Folds')\n",
    "ax2.set_xlabel('Fold')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4366744,
     "sourceId": 7499077,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4790312,
     "sourceId": 8109577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4880800,
     "sourceId": 8230342,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5307425,
     "sourceId": 8822174,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 172069479,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174052124,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175828211,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 185952152,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186208771,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
